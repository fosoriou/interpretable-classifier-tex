@article{Breiman2001,
   abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * *, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
   author = {Leo Breiman},
   doi = {10.1023/A:1010933404324},
   issn = {08856125},
   issue = {1},
   journal = {Machine Learning},
   title = {Random forests},
   volume = {45},
   year = {2001},
}
@book{Fernandez2018,
   author = {Alberto Fernández and Salvador García and Mikel Galar and Ronaldo C. Prati and Bartosz Krawczyk and Francisco Herrera},
   city = {Cham},
   doi = {10.1007/978-3-319-98074-4},
   isbn = {978-3-319-98073-7},
   publisher = {Springer International Publishing},
   title = {Learning from Imbalanced Data Sets},
   url = {http://link.springer.com/10.1007/978-3-319-98074-4},
   year = {2018},
}
@book{Breiman1984,
   author = {Leo Breiman and Jerome H. Friedman and Richard A. Olshen and Charles J. Stone},
   doi = {10.1201/9781315139470},
   isbn = {9781315139470},
   month = {10},
   publisher = {Routledge},
   title = {Classification And Regression Trees},
   year = {1984},
}
@article{Pedregosa2011,
   abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
   author = {Fabian Pedregosa and Gael Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and Édouard Duchesnay},
   journal = {Journal of Machine Learning Research},
   keywords = {Python,model selection,supervised learning,unsupervised learning},
   pages = {2825-2830},
   title = {Scikit-learn: Machine Learning in Python},
   volume = {12},
   url = {http://jmlr.org/papers/v12/pedregosa11a.html},
   year = {2011},
}
@article{Quinlan1986,
   abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to s...},
   author = {J. R. Quinlan},
   doi = {10.1023/A:1022643204877},
   issn = {15730565},
   issue = {1},
   journal = {Machine Learning},
   keywords = {classification,decision trees,expert systems,induction,information theory,knowledge acquisition},
   month = {3},
   pages = {81-106},
   publisher = {Kluwer Academic Publishers},
   title = {Induction of Decision Trees},
   volume = {1},
   url = {https://dl.acm.org/doi/10.1023/A%3A1022643204877},
   year = {1986},
}
@book{Quinlan1993,
   author = {J. Ross Quinlan},
   city = {San Francisco, CA, USA},
   doi = {https://dl.acm.org/doi/book/10.5555/152181},
   isbn = {1558602380},
   publisher = {Morgan Kaufmann Publishers Inc.},
   title = {C4.5: Programs for Machine Learning},
   year = {1993},
}
@article{Breiman1996,
   abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
   author = {Leo Breiman},
   doi = {10.1007/BF00058655},
   issn = {1573-0565},
   issue = {2},
   journal = {Machine Learning 1996 24:2},
   keywords = {Artificial Intelligence,Control,Mechatronics,Natural Language Processing (NLP),Robotics,Simulation and Modeling},
   pages = {123-140},
   publisher = {Springer},
   title = {Bagging predictors},
   volume = {24},
   url = {https://link.springer.com/article/10.1007/BF00058655},
   year = {1996},
}
@article{Bikku2022,
   author = {Dr Thulasi Bikku and Felipe Herrera},
   doi = {10.5281/ZENODO.6419971},
   month = {4},
   title = {fherreralab/organic_optical_classifier: v1.0.0},
   url = {https://zenodo.org/record/6419971},
   year = {2022},
}
@article{Bikku2022,
   abstract = {Identifying chemical compounds is essential in several areas of science and engineering. Laser-based techniques are promising for autonomous compound detection because the optical response of materials encodes enough electronic and vibrational information for remote chemical identification. This has been exploited using the fingerprint region of infrared absorption spectra, which involves a dense set of absorption peaks that are unique to individual molecules, thus facilitating chemical identification. However, optical identification using visible light has not been realized. Using decades of experimental refractive index data in the scientific literature of pure organic compounds and polymers over a broad range of frequencies from the ultraviolet to the far-infrared, we develop a machine learning classifier that can accurately identify organic species based on a single-wavelength dispersive measurement in the visible spectral region, away from absorption resonances. The optical classifier proposed here could be applied to autonomous material identification protocols or applications.},
   author = {Thulasi Bikku and Rubén A. Fritz and Yamil J. Colón and Felipe Herrera},
   doi = {10.1021/acs.jpca.2c07955},
   issue = {10},
   journal = {Journal of Physical Chemistry A},
   keywords = {Classification,Machine Learning * Corresponding author: felipeherrerau@usachcl 2,Organic Compounds,Random Forest,Refractive Index},
   month = {4},
   pages = {2407-2414},
   publisher = {American Chemical Society},
   title = {Machine learning identification of organic compounds using visible light},
   volume = {127},
   url = {http://arxiv.org/abs/2204.11832 http://dx.doi.org/10.1021/acs.jpca.2c07955},
   year = {2022},
}
@article{Roscher2020,
   abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.},
   author = {Ribana Roscher and Bastian Bohn and Marco F. Duarte and Jochen Garcke},
   doi = {10.1109/ACCESS.2020.2976199},
   issn = {21693536},
   journal = {IEEE Access},
   title = {Explainable Machine Learning for Scientific Insights and Discoveries},
   volume = {8},
   year = {2020},
}
@book{Hastie2009,
   abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
   author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
   issn = {01727397},
   issue = {2},
   journal = {Elements},
   title = {Elements of Statistical Learning 2nd ed.},
   volume = {27},
   year = {2009},
}
@article{Reback2022,
   author = {Jeff Reback and jbrockmendel and Wes McKinney and Joris Van den Bossche and Tom Augspurger and Matthew Roeschke and Simon Hawkins and Phillip Cloud and gfyoung and Sinhrks and Patrick Hoefler and Adam Klein and Terji Petersen and Jeff Tratner and Chang She and William Ayd and Shahar Naveh and JHM Darbyshire and Marc Garcia and Richard Shadrach and Jeremy Schendel and Andy Hayden and Daniel Saxton and Marco Edward Gorelli and Fangchen Li and Matthew Zeitlin and Vytautas Jancauskas and Ali McMaster and Torsten Wörtwein and Pietro Battiston},
   doi = {10.5281/ZENODO.6408044},
   month = {4},
   title = {pandas-dev/pandas: Pandas 1.4.2},
   url = {https://doi.org/10.5281/zenodo.6408044#.Yqvq9bRvXHI.mendeley},
   year = {2022},
}
